# Reviewer: 1

## Comments to the Author
This is a well organised and written manuscript, describing a very thorough investigation of a reasonably interesting problem. Computational aspects of complex adaptive clinical trial designs have all too often been neglected. The speed gain proposed here is substantial, with no apparent loss in quality. The findings could help improve the uptake of adaptive seamless designs. I only have a few relatively minor suggestions that I hope the authors will find helpful.

## General

1) Power is used as a key performance metric, yet it lacks an explicit definition. The problem, of course, is that power is slightly more complicated in a multi-stage design with multiple arms (and therefore multiple hypotheses to be tested) than it is in a simple comparison of two groups. Given the scenario described in the motivating example, I would have assumed that power was defined as the probability of at least one (any) treatment group being demonstrated to be ‘better’ than control i.e. a ‘disjunctive’ or ‘minimal’ power definition (cf. Senn & Bretz 2007). However, on p. 10 it is stated that “the vector of treatment numbers for determining power counts rejections of 3 or 4 hypotheses”, implying a power definition more similar to ‘conjunctive’ or ‘maximal’ power.

Reference: Senn S, Bretz F (2007) Power and sample size when multiple endpoints are considered. Pharmaceutical Statistics, 6(3), 161-170.

--> Tim Friede

2) There is some inconsistent notation:

a) It seems that n_treat is varyingly defined as the “total number of treatments” (p. 8, l. 29 and p. 9, l. 20), “total number of patients” (p. 8, l. 48 and p. 9, l. 14) and “allowed treatment number” (p. 12, l. 50 and p. 12, l. 51).

b) On p. 3 k is introduced as the running index for treatments (k = 1, …, K) but later on p. 9 k is the resolution of the search grid.

Specific

p. 2: For the benefit of readers less familiar with the term, the authors should explain at least briefly what they mean by “expensive black-box” optimisation problems.

p. 3 ff.: This may be implicitly clear but it should also be said explicitly that in the motivating example and throughout the remainder of the paper a normality assumption is made for all outcome data.

p. 9: How were the values for the resolution of Grid and Grid Small (k=25 and k=7) selected?

--> Check other papers, otherwise rule of thumb, on previous results we saw that 25 should suffice to find the global optimum, k = 7 because it saves recalculations

p. 10: Similarly, how was the number of randomly sampled points (16) for the initial design for BO chosen?

--> Rule of thumb 4*d

p. 10: In section 5.4 ten additional stochastic simulations are mentions, which seems in conflict with the 20 additional stochastic simulations mentioned in other parts of the manuscript.

p. 10: It should be clarified whether the test level of 0.025 is one-sided.

p. 11: Whilst I appreciate that the focus is on relative improvements in runtime, the authors should at least briefly describe the hardware setup with which these runtimes were achieved, to provide some context for the absolute runtimes.

p. 12: It is stated that: “Only for the scenarios (effect: paper2, n_treat = 1000) and (effect: sigmoid, n_treat = 1000) Grid Small has a small advantage over BO and Grid.” I think it should be added that Grid Small also has an advantage over BO for (effect: paper, n_treat = 1000) and certainly for (effect: sigmoid, n_treat = 2000).

p. 12: It says: “For example, for scenario effect: sigmoid, n_treat = 1000 Grid is superior to BO, but the difference is smaller than 0.25%.” This isn’t true for n_treat = 1000 where the difference between Grid and BO is around 2%, but rather for n_treat = 2000.

p. 12: Apologies but I’m struggling to understand the following sentence: “As intuitively anticipated, in almost all scenarios the worst results were obtained if all available treatments are used in stage 1 and none in stage 2.” How can none of the treatments be used in stage 2 when on p. 8 it is specified that the number of treatments in stage 2 must always be between 2 and 5?

--> clarify

p. 13: It is stated that: “Note, that the selection strategies thresh and all never selected powerful designs.” Why is that? Presumably, the former is true because the range of numerical values used for the threshold was chosen too narrow, whilst the latter shouldn’t be surprising at all, as it is the only non-adaptive design in the mix.

--> Maybe threshold is disadvantageous because it looks at the absolute values, which is not that adaptive even with optimization 

p. 14: What are the x-values referred to in the caption of Figure 4? No x is introduced in the notation, and Figure 4 itself plots y-values against r-values.

p. 16: The citation of the technical report by Bischl et al. should include a URL if possible.

Typos

p. 9, l. 20: “in average”, which should read “on average”

p. 9, l. 24: “in 8”, which should read “in (8)”

p. 9, l. 53 and p. 10, l. 14: “Mattern”, which should be spelt “Matérn”

p. 10, l. 28: “Gird”

p. 10, l. 39: “???”

p. 12, l. 19: “Gris”

p. 13, l. 48f.: remove closing bracket from “500)”

p. 14, l. 55: “which conflicts the goal is to find the optimal” (grammar)

Reviewer: 2

Comments to the Author
The authors propose to use Bayesian optimization (BO) to improve the efficiency of the design selection process in clinical trials. A set of parameters to be chose for the design optimization based on the power. The idea seem novel and could be a useful approach. However, there are several major issues and limitations of proposed methods: for example, power is often only part of measure of choosing design, other factors like study duration, and # of pts are also important. Also, the black-box function relay too much of the various parametric assumption of parameters, which need to be carefully decided to be more related to true clinical trial needs. Overall, I have the following comments for the authors.

--> In out method we keep the number of patients fixed
--> Tim Friede (assumption of parameters, true clinical trial needs)

Major comments:

In page 7 table 1, the effect size scenarios presented seem always assume 2nd stage effect size are higher than stage 1? The authors should also evaluate the vice visa situation, and consistent effect size case in simulation.

--> Argument: Falschrum
--> Often Realisitic scenario, also for the motivating example: First stage, second stage ...
--> Tim

In page 8 table 2, the selection of “espilon” and “threshold” used in the arm selection should be based on the clinical meaningful cut, I am not sure how practical that they are part of parameters to be optimized? The authors should also evaluate with different setting.

--> Argument: Given a simulation setting we optimize the parameters eps and thr. This is independent of the application but of course the clinical application can give hints which simulation settings are adequate

In section 5.2, the authors keep the total number of pts fixed, which seem not appropriate in practice, as overall power relying several factors, e.g. # of pts, the interim selection rule, stage 1 and stage 2 ratio. Often # of pts is also key goal of optimal design evaluation. I will strongly suggest # of pts should be a parameter in the black-box function, instead of fixing it to a constant value.

--> Argument: This will only lead to a result where more patients are selected. The idea is to run this optimization for different patient sizes, and then to decide which number of patients is required (also mention this in paper)

In page 10 table 4, the number of simulation iteration is only 1000, which does not seem sufficient for complex optimization methods proposed by authors. I would suggest at least 5000 should be needed.

--> simulation iteration means that we evaluated 1000 times wheter the test detected a significant effect. this does not depend on the complexity of the optimization. As a matter of fact we use 100 optimization iterations which is sufficient as can be seen in plot ... where we reach convergence.

Page 11 table 5, the authors present the average run time in hour, not sure how these information is helpful, as this depend on the what computing systems were used in the simulation. Please either add more information.

--> add computational information, relative values are still meaningfull

Page 13, BO has issue to find optimal value when one parameters closer to the borders of search space. This is concerning and confirm the limitation of using too many assumptions for optimization, e.g. appropriate parameters range. I am not sure taking log-transform could solve this problem, could authors elaborate more details on this.  Additional simulation is necessary to confirm this.   

--> log transform: good idea, outlook


Given authors used COPD trial as motivating example, I would strongly suggest to add section of case study for the illustration of proposed methods.

--> in fact our effect set "paper" issues a case study where the effect sets are estimated on a true dataset. we do not propose a method to analyze data, but a method to select the best design given a known theoretical situation. (check that this is clear)

Minor comments:

Page 8 line 29,  n_{treat} should be total # of pts, instead of “total number of treatment”.